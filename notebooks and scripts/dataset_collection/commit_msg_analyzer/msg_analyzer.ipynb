{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d76658",
   "metadata": {},
   "source": [
    "# Set the target repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04420077-d6d0-470f-8f3a-ae1277bc9a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project name and URL link to it.\n",
    "project_name = \"elasticsearch\"\n",
    "git_repo_url = \"https://github.com/elastic/elasticsearch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64d6f51",
   "metadata": {},
   "source": [
    "# Installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade nltk\n",
    "!pip install --upgrade pathvalidate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd258d",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e3c283-172a-4b91-83b6-48b390c74eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from pathvalidate import is_valid_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5a891",
   "metadata": {},
   "source": [
    "# Define analysis settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604dbaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.4  # Messages with a count less than this value are not saved.\n",
    "save_details = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c0c7a6",
   "metadata": {},
   "source": [
    "# 1. Preparing data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a28e79",
   "metadata": {},
   "source": [
    "## 1.1. We receive all commit messages from the specified repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624be697",
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh = True\n",
    "if os.path.exists(\"repo\"):\n",
    "    print(\"local 'repo' folder found. reuse [Y/n]? \", end=\" \")\n",
    "    user_input = input()\n",
    "    if user_input.lower() == \"y\" or user_input == \"\":\n",
    "        fresh = False\n",
    "        \n",
    "# If an update is required, we clone the repository.\n",
    "if fresh:\n",
    "    print(\"clone into 'repo'\")\n",
    "    if not os.path.exists(\"repo\"):\n",
    "        os.makedirs(\"repo\")\n",
    "    subprocess.run([\"git\", \"clone\", git_repo_url, \"repo\"])\n",
    "else:\n",
    "    print(\"reuse local 'repo'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca40d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_separator = \"^@@@^\"\n",
    "line_separator = \"^&_@&_@&_@^\"\n",
    "git_format = \"--pretty=format:%H\" + field_separator + \"%s%n%b\" + line_separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run git log and get the output.\n",
    "proc = subprocess.run([\"git\", \"log\", git_format], cwd='repo/', stdout=subprocess.PIPE)\n",
    "raw_data = proc.stdout.decode(\"iso-8859-1\")\n",
    "# We split the output into elements (hash and commit message).\n",
    "item_list = raw_data.split(line_separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b106ba-b013-4135-9531-1ab33d73c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store hashes and commit messages.\n",
    "res = []\n",
    "for item in item_list:\n",
    "    item = item.strip()\n",
    "    if item.find(field_separator) > 0:\n",
    "        parts = item.split(field_separator)\n",
    "        commit_hash = parts[0].strip()\n",
    "        msg = parts[1].strip()\n",
    "        res.append([commit_hash, msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee2f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a message about the number of loaded commit messages.\n",
    "print(\"* %d commit messages dumped\" % len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9c2247",
   "metadata": {},
   "source": [
    "## 1.2. Filtering some patterns from commit messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa047c90-a1f1-4497-aa11-1bbbafe9ba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_filtered = []\n",
    "for each_instance in res:\n",
    "    id_, msg_ = each_instance\n",
    "    msg_list = msg_.split(\"\\n\")\n",
    "    filtered = []\n",
    "    for sent_ in msg_list:\n",
    "        sent_ = sent_.strip('\\t\\n\\r\\f\\v').strip()\n",
    "        # Exclude lines containing the following patterns:\n",
    "        # 1. The \"-[Bb][Yy]:\" pattern, which is often used to indicate the author of the changes (by, By, bY, BY);\n",
    "        # 2. The \"[Cc][Cc]:\" pattern, which can indicate a copyright or some kind of contact address (Cc, CC, cc, cC);\n",
    "        # 3. The pattern \"[Hh][Tt][Tt][Pp]s?:\", which usually points to a link (HTTP, https, Http, http);\n",
    "        # 4. Empty lines (len(sent_) == 0).\n",
    "        if re.search('-[Bb][Yy]:', sent_) or re.search('[Cc][Cc]:', sent_) or re.search('[Hh][Tt][Tt][Pp]s?:', sent_) or len(sent_) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            filtered.append(sent_)\n",
    "    res_filtered.append([id_, msg_, filtered])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3426ce",
   "metadata": {},
   "source": [
    "## 1.3. We process the original tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6425139",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords', download_dir=\"nltk_data\")\n",
    "nltk.download('punkt', download_dir=\"nltk_data\")\n",
    "nltk.data.path.append(\"nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666e7e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the remaining lines except the first.\n",
    "filtered_token = []\n",
    "for each in res_filtered:\n",
    "    id_, old, new = each\n",
    "    if len(new) >= 2:\n",
    "        other_sentence = \" \".join(new[1:])\n",
    "        other_sentence = other_sentence.strip('\\t\\n\\r\\f\\v').strip().lower()\n",
    "        # Tokenize the string using the NLTK library.\n",
    "        tokens = nltk.word_tokenize(other_sentence)\n",
    "        # Exclude punctuation marks and numbers from the list of tokens.\n",
    "        tokens = [tok for tok in tokens if tok.isalpha()]\n",
    "        # Remove stopwords from the list of tokens.\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # Remove tokens containing less than three characters.\n",
    "        tokens = [w for w in tokens if len(w) > 2]\n",
    "        if len(tokens) > 0:\n",
    "            filtered_token.append([id_, old, tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66f6df",
   "metadata": {},
   "source": [
    "# 2. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a9129",
   "metadata": {},
   "source": [
    "# 2.1. We read keywords from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40fc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_dict = dict()\n",
    "with open(\"keywords.csv\", encoding=\"utf8\") as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    birth_header = next(csv_reader)\n",
    "    for row in csv_reader:  \n",
    "        class_ = row[0]\n",
    "        words = row[1]\n",
    "        score = row[2]\n",
    "        if class_ != \"\":\n",
    "            cur_class = class_\n",
    "            keyword_dict[cur_class] = dict()\n",
    "        word_list = words.split(\"/\")\n",
    "        for each in word_list:\n",
    "            keyword_dict[cur_class][each] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272b1d50",
   "metadata": {},
   "source": [
    "# 2.2. We obtain all proposals from pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cd6c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for each in filtered_token:\n",
    "    id_, org, s = each\n",
    "    sentences.append([id_, \" \".join(s), org])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8763f1b",
   "metadata": {},
   "source": [
    "# 2.3. We calculate initial estimates for each proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6e5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "all_original_msgs = []\n",
    "for each in sentences:  \n",
    "    s = 0\n",
    "    id_, each_s, org_s = each\n",
    "    classes, key_words = [], []\n",
    "    for each_class in keyword_dict:  \n",
    "        cur_class = keyword_dict[each_class]\n",
    "        for each_key in cur_class:  \n",
    "            c_sc = float(cur_class[each_key])  \n",
    "            if each_s.find(each_key) >= 0:  \n",
    "                s += c_sc \n",
    "                classes.append(each_class)\n",
    "                key_words.append(each_key)\n",
    "    scores.append([id_, s, classes, key_words, each_s, org_s])\n",
    "    all_original_msgs.append(org_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914b98e",
   "metadata": {},
   "source": [
    "## 2.4 We process sentences and assign tags to them based on keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97deb752",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tagged = []\n",
    "total = len(all_original_msgs)\n",
    "idx = 0 \n",
    "for each in all_original_msgs:\n",
    "    # Analyze each message using a keyword dictionary.\n",
    "    new_msg = []\n",
    "    sentences = each.split(\"\\n\")\n",
    "    for each_sentence in sentences:\n",
    "        each_sentence = each_sentence.strip('\\t\\n\\r\\f\\v').strip().lower()\n",
    "        tokens = each_sentence.split(\" \")\n",
    "        new_tokens = []\n",
    "        for tok in tokens:\n",
    "            tok = tok.strip(\"\\t\\n\\r\\f\\v.:',;\")\n",
    "            if tok == \"\":\n",
    "                continue\n",
    "            if tok.isalpha():\n",
    "                # Checking the token for presence in the key dictionary.\n",
    "                category = None\n",
    "                for each_cate in keyword_dict:\n",
    "                    cur_dict = keyword_dict[each_cate]\n",
    "                    if tok in cur_dict:\n",
    "                        category = each_cate\n",
    "\n",
    "                if category:\n",
    "                    tok = \"<keyword category=\" + category + \">\" + tok + \"</keyword>\"\n",
    "                new_tokens.append(tok)\n",
    "            else:\n",
    "                if \"(\" in tok and tok.endswith(\")\"):\n",
    "                    parts = tok[: -1].split(\"(\")\n",
    "                    if parts[0].isidentifier():\n",
    "                        tok = \"<function>\" + tok + \"</function>\"\n",
    "                elif \".\" in tok:\n",
    "                    if is_valid_filepath(tok):\n",
    "                        _, f_ext = os.path.splitext(tok)\n",
    "                        if f_ext == \".c\" or f_ext == \".h\" or f_ext == \".cpp\" or f_ext == \".hpp\":\n",
    "                            tok = \"<file_name>\" + tok + \"</file_name>\"\n",
    "                elif tok.isidentifier():\n",
    "                    tok = \"<variable>\" + tok + \"</variable>\"\n",
    "                new_tokens.append(tok)\n",
    "        new_msg.append(new_tokens)\n",
    "    \n",
    "    all_tagged.append(new_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b921b0bc",
   "metadata": {},
   "source": [
    "## 2.5. We collect tags and combine them for each message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cecbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest(key_index, cur_set, set_indexes):\n",
    "    \"\"\"\n",
    "    Returns the closest element from the given set to the specified index.\n",
    "\n",
    "    :param key_index: Key element index;\n",
    "    :param cur_set: Set of elements;\n",
    "    :param set_indexes: List of indexes of elements from the set.\n",
    "    :return: The closest element to the specified index.\n",
    "    \"\"\"\n",
    "    near = None\n",
    "    if len(cur_set) == 0:\n",
    "        pass\n",
    "    elif len(cur_set) == 1:\n",
    "        near = cur_set[0]\n",
    "    else:\n",
    "        val = abs(key_index - set_indexes[0])\n",
    "        index = 0 \n",
    "        for i in range(1, len(set_indexes)):\n",
    "            cur_val = abs(key_index - set_indexes[i])\n",
    "            if cur_val < val:\n",
    "                index = i\n",
    "        near = cur_set[index]\n",
    "    return near"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed1da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_variable(tok):\n",
    "    new_tok = tok[len('<variable>'): tok.find('</')]\n",
    "    return new_tok\n",
    "def clean_function(tok):\n",
    "    new_tok = tok[len('<function>'): tok.find('</')]\n",
    "    return new_tok\n",
    "def clean_filename(tok):\n",
    "    new_tok = tok[len('<file_name>'): tok.find('</')]\n",
    "    return new_tok\n",
    "\n",
    "def compute_pattern(key, key_index, variables, variable_indexes, functions, function_indexes, files,\n",
    "                    file_indexes):\n",
    "    \"\"\"\n",
    "    Computes a pattern for a tag.\n",
    "\n",
    "    :param key: Keyword;\n",
    "    :param key_index: Index of the keyword in the sentence;\n",
    "    :param variables: List of variables;\n",
    "    :param variable_indexes: Indexes of variables in the clause;\n",
    "    :param functions: List of functions;\n",
    "    :param function_indexes: Function indexes in the sentence;\n",
    "    :param files: List of files;\n",
    "    :param file_indexes: File indexes in the sentence.\n",
    "    :return: key, category, nearest variable, nearest function, nearest file.\n",
    "    \"\"\"\n",
    "    category = key[len('<keyword category='): key.find('>')]\n",
    "    key = key[key.find('>') + 1: key.find('</')]\n",
    "\n",
    "    near_var = get_nearest(key_index, variables, variable_indexes)  # Finding the closest variable.\n",
    "    near_fun = get_nearest(key_index, functions, function_indexes)  # Finding the closest function.\n",
    "    near_file = get_nearest(key_index, files, file_indexes)  # Finding the nearest file.\n",
    "    if near_var is not None:\n",
    "        near_var = clean_variable(near_var)\n",
    "    if near_fun is not None:\n",
    "        near_fun = clean_function(near_fun)\n",
    "    if near_file is not None:\n",
    "        near_file = clean_filename(near_file)\n",
    "    return key, category, near_var, near_fun, near_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4567c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_collected_tags(collected_tags):\n",
    "    \"\"\"\n",
    "    Combines collected tags.\n",
    "    \n",
    "    :param collected_tags: List of collected tags.\n",
    "    :return: List of new combination tags.\n",
    "    \"\"\"\n",
    "    new_combined = []\n",
    "    if len(collected_tags) == 0:\n",
    "        pass\n",
    "    elif len(collected_tags) == 1:\n",
    "        key_word, category, sentence, variable, function, file = collected_tags[0]\n",
    "        new_ins = [category, sentence, [key_word], [variable], [function], [file]]\n",
    "        new_combined.append(new_ins)\n",
    "    else:\n",
    "        start = collected_tags[0]\n",
    "        kw_list, variable_list, function_list, file_list = [], [], [], []\n",
    "        s_key_word, s_category, s_sentence, s_variable, s_function, s_file = start\n",
    "        # Adding tags.\n",
    "        kw_list.append(s_key_word)\n",
    "        variable_list.append(s_variable)\n",
    "        function_list.append(s_function)\n",
    "        file_list.append(s_file)\n",
    "        for index in range(1, len(collected_tags)):\n",
    "            cur_instance = collected_tags[index]\n",
    "            kw, cate, sent, var, fun, fil = cur_instance\n",
    "            if cate == s_category and sent == s_sentence:\n",
    "                kw_list.append(kw)\n",
    "                variable_list.append(var)\n",
    "                function_list.append(fun)\n",
    "                file_list.append(fil)\n",
    "                if index == len(collected_tags) - 1:\n",
    "                    new_combined.append([s_category, s_sentence, kw_list, variable_list, function_list, file_list])\n",
    "            else:\n",
    "                new_combined.append([s_category, s_sentence, kw_list, variable_list, function_list, file_list])\n",
    "                s_category = cate\n",
    "                s_sentence = sent\n",
    "                kw_list = [kw]\n",
    "                variable_list = [var]\n",
    "                function_list = [fun]\n",
    "                file_list = [fil]\n",
    "                if index == len(collected_tags) - 1:\n",
    "                    new_combined.append([s_category, s_sentence, kw_list, variable_list, function_list, file_list])\n",
    "    return new_combined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5693f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_strings(new_combined):\n",
    "    \"\"\"\n",
    "    Generates strings based on combined tags.\n",
    "    \n",
    "    :param new_combined: List of combination tags.\n",
    "    :return: List of strings generated from combination tags.\n",
    "    \"\"\"\n",
    "    string_list = []\n",
    "    if len(new_combined) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        # Iterate through each combination tag.\n",
    "        for each in new_combined:  \n",
    "            category, sentence, kws, varis, funs, files = each\n",
    "            # Remove duplicates.\n",
    "            kws = list(set(kws))\n",
    "            varis = list(set(varis))\n",
    "            funs = list(set(funs)) \n",
    "            files = list(set(files))\n",
    "            # Remove None from the list.\n",
    "            if None in kws:\n",
    "                kws.remove(None)  \n",
    "            if None in varis:\n",
    "                varis.remove(None) \n",
    "            if None in funs:\n",
    "                funs.remove(None) \n",
    "            if None in files:\n",
    "                files.remove(None) \n",
    "            cur_str = \"<Category: \" + category + \"> \" + \"<Sentence: \" + sentence + \"> \" + \"<Keyword: \"+ \" \".join(kws) + \"> \"\n",
    "            # Formation of a string with category, sentence and keywords.\n",
    "            if len(varis) > 0:\n",
    "                cur_str += \"<Variable: \" + \" \".join(varis) + \"> \"\n",
    "            if len(funs) > 0:\n",
    "                cur_str += \"<Function: \" + \" \".join(funs) + \"> \"\n",
    "            if len(files) > 0:\n",
    "                cur_str += \"<File: \" + \" \".join(files) + \">\"\n",
    "            string_list.append(cur_str)\n",
    "    return string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60866c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = {\n",
    "    \"all_none\": 0,\n",
    "    \"nonkey_withother\": 0,\n",
    "    \"withkey_nonother\": 0,\n",
    "    \"withkey_withother\": 0\n",
    "}\n",
    "collected_tags = []\n",
    "for msg_index, each_msg in enumerate(all_tagged):\n",
    "    cur_collected_tags = []\n",
    "    for sent_index, each_sentence in enumerate(each_msg):\n",
    "        keywords = [] \n",
    "        variables, functions, files = [], [], []\n",
    "        keyword_indexes, variable_indexes, function_indexes, file_indexes = [], [], [], []\n",
    "        for index, each_tok in enumerate(each_sentence):\n",
    "            if \"<keyword\" in each_tok:\n",
    "                keywords.append(each_tok)\n",
    "                keyword_indexes.append(index)\n",
    "            elif \"<variable>\" in each_tok:\n",
    "                variables.append(each_tok)\n",
    "                variable_indexes.append(index)\n",
    "            elif \"<function>\" in each_tok:\n",
    "                functions.append(each_tok)\n",
    "                function_indexes.append(index)\n",
    "            elif \"<file_name>\" in each_tok:\n",
    "                files.append(each_tok)\n",
    "                file_indexes.append(index)\n",
    "            else:\n",
    "                pass\n",
    "        if len(keywords) > 0:\n",
    "            if len(keywords) == 1:\n",
    "                if len(variables) == 0 and len(functions) == 0 and len(files) == 0:\n",
    "                    statistics[\"withkey_nonother\"] += 1\n",
    "                else:\n",
    "                    statistics[\"withkey_withother\"] += 1\n",
    "                    key = keywords[0]\n",
    "                    key_index = keyword_indexes[0]\n",
    "                    key, cate, var, fun, fil = compute_pattern(key, key_index, variables, variable_indexes,\n",
    "                                                                functions, function_indexes, files,\n",
    "                                                                file_indexes)\n",
    "                    ori_sent = all_original_msgs[msg_index][sent_index]\n",
    "                    cur_collected_tags.append((key, cate, ori_sent, var, fun, fil))\n",
    "            elif len(keywords) > 1:\n",
    "                if len(variables) == 0 and len(functions) == 0 and len(files) == 0:\n",
    "                    statistics[\"withkey_nonother\"] += 1\n",
    "                else:\n",
    "                    statistics[\"withkey_withother\"] += 1\n",
    "                    for key_i in range(len(keywords)):\n",
    "                        key = keywords[key_i]\n",
    "                        key_index = keyword_indexes[key_i]\n",
    "                        key, cate, var, fun, fil = compute_pattern(key, key_index, variables,\n",
    "                                                                    variable_indexes,\n",
    "                                                                    functions, function_indexes, files,\n",
    "                                                                    file_indexes)\n",
    "                        ori_sent = all_original_msgs[msg_index][sent_index]\n",
    "                        cur_collected_tags.append((key, cate, ori_sent, var, fun, fil))\n",
    "        else:\n",
    "            if len(variables) == 0 and len(functions) == 0 and len(files) == 0:\n",
    "                statistics[\"all_none\"] += 1\n",
    "            elif len(variables) > 0 or len(functions) > 0 or len(files) > 0:\n",
    "                statistics[\"nonkey_withother\"] += 1\n",
    "    new_combined = combine_collected_tags(cur_collected_tags)  # Combining collected tags.\n",
    "    string_represts = compute_strings(new_combined)  # Getting a string representation of tags.\n",
    "    collected_tags.append(string_represts)  # Adding a string representation to the list of collected tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb3bdc4",
   "metadata": {},
   "source": [
    "# 2.6. We save the analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8906d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d\")  # Current time in YYYYMMDD format.\n",
    "commit_cnt = 0\n",
    "output_path = \"result\"  # Folder for saving results.\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Saving hashes of commits with positive scores in a separate file.\n",
    "with open(os.path.join(output_path, \"%s_%s_commit_only.txt\" % (timestamp, project_name)), \"w\") as fp:\n",
    "    for index, each in enumerate(scores):\n",
    "        commit_hash, score, _, _, _, _ = each\n",
    "        if score > threshold:\n",
    "            fp.write(commit_hash + \"\\n\")\n",
    "            commit_cnt += 1\n",
    "print(\"* %d commits with positive scores identified\" % commit_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf6defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving commit hashes and their scores to a CSV file.\n",
    "if save_details:\n",
    "    with open(os.path.join(output_path, \"%s_%s_commit_score.csv\" % (timestamp, project_name)), \"w\") as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow([\"commit_hash\", \"score\"])\n",
    "        for index, each in enumerate(scores):\n",
    "            commit_hash, score, _, _, _, _ = each\n",
    "            writer.writerow([commit_hash, score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e97bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the save_details flag is set to True, additional analysis details are saved.\n",
    "if save_details:\n",
    "    def remove_repeat(old_list):\n",
    "        list1 = []\n",
    "        for element in old_list:\n",
    "            if (element not in list1):\n",
    "                list1.append(element)\n",
    "        return list1\n",
    "    \n",
    "    with open(os.path.join(output_path, \"%s_%s_details.csv\" % (timestamp, project_name)), \"w\") as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(\n",
    "            [\"hashcode\", \"score\", \"class\", \"keyword\", \"msg\", \"original msg\"] + [\"tagged\", \"pattern\"])\n",
    "        for index, each in enumerate(scores):\n",
    "            id_, sc, classes, key_words, each_s, each_o = each\n",
    "            original_writer = [id_, sc, \"**\".join(remove_repeat(classes)),\n",
    "                               \"**\".join(remove_repeat(key_words)), each_s, each_o]\n",
    "            tagged = all_tagged[index]\n",
    "            new_string_tagged = []\n",
    "            for each_sentence in tagged:\n",
    "                new_string_tagged.append(\" \".join(each_sentence))\n",
    "            tagged_new = \"\\n\".join(new_string_tagged)\n",
    "            if len(collected_tags[index]) > 0:\n",
    "                pattern_new = \"\\n\".join(collected_tags[index])\n",
    "                writer.writerow(original_writer + [tagged_new, pattern_new])\n",
    "            else:\n",
    "                writer.writerow(original_writer + [tagged_new])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
