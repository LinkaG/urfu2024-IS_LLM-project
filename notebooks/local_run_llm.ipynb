{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import accelerate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading samples from a file \"vul_sample.csv\"\n",
    "df = pd.read_csv(\"../data/vul_sample.csv\")\n",
    "parsed_data = df[[\"CWE ID\", \"func_before\", \"len\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stabilityai/stable-code-3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up for GPU offload\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Quantization to 8 bits or 4 bits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a model with certain parameters\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,  # FP32 or FP16\n",
    "    # quantization_config=config,  # Enable if there is not enough VRAM\n",
    "    device_map=\"cuda\",  # Loading the model and tokenizer on CUDA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a tokenizer for a model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline for text generation\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64, temperature=0.3, do_sample=True)\n",
    "\n",
    "# Creating a HuggingFacePipeline with a Pipeline\n",
    "bloom = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for question and answer\n",
    "template = \"\"\"System: You are a security researcher, expert in detecting security vulnerabilities. Provide response only in following format: vulnerability: <YES or NO> | vulnerability type: <CWE ID> | explanation: <explanation for prediction>. Use N/A in other fields if there are no vulnerabilities. Do not include anything else in response.\n",
    "User: Evaluate the security of the following code snippet for potential vulnerabilities:\n",
    "{vulnerable_code}\n",
    "\n",
    "Response:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"vulnerable_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=bloom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Running iterations over all sals from a parsed_data\n",
    "for index, row in tqdm(parsed_data.iterrows(), total=len(parsed_data), desc=\"Processing\", unit=\" row\"):\n",
    "    result = {}\n",
    "    \n",
    "    # Sending a question to a model\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        generated_text = llm_chain.invoke(row[\"func_before\"])['text'].strip()\n",
    "        end_time = time.time()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    print(f\"Expected CWE ID: {row['CWE ID']}. The model answers:\\n{generated_text}\")\n",
    "    print(f\"\\nExecution time: {(end_time - start_time):.2f} seconds\")\n",
    "        \n",
    "    result[\"Expected CWE ID:\"] = row[\"CWE ID\"]\n",
    "    result[\"generated_text\"] = generated_text\n",
    "    result[\"lead_time\"] = end_time - start_time\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "# Saving results to a JSON file\n",
    "model_name = model_id.split(\"/\")[1]\n",
    "output_file = f\"../data/collected_generated_text/results_{model_name}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4636248,
     "sourceId": 7895548,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
